{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Arthur-ca/MIE1517/blob/YayunYang/Project_small.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:40:11.603938107Z",
     "start_time": "2023-11-12T19:40:09.387315148Z"
    },
    "id": "yq0MKgApDInX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import torchvision\n",
    "import time\n",
    "import cv2 as cv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as F1\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import imgaug.augmenters as iaa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, models, transforms\n",
    "from collections import defaultdict\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:40:11.612871727Z",
     "start_time": "2023-11-12T19:40:11.605457967Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTd6YA73JsJC",
    "outputId": "11591e23-0766-4fcc-b198-d7831c3014c3"
   },
   "outputs": [],
   "source": [
    "directory_path = '/content/gdrive/MyDrive/archive'\n",
    "ds_store_path = os.path.join(directory_path, '.DS_Store')\n",
    "\n",
    "if os.path.exists(ds_store_path):\n",
    "    os.remove(ds_store_path)\n",
    "else:\n",
    "    print(\".DS_Store file does not exist in the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:40:11.623339999Z",
     "start_time": "2023-11-12T19:40:11.607784429Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "PjxPEAukDNt-",
    "outputId": "0434ef2d-373a-4498-8a0f-1f57f4f16bf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'contempt', 'disgust', 'fear', 'happiness', 'neutrality', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "#First link it to google drive and check the subfolder of the data\n",
    "# data_path = '/home/qinghao/backup/MIE1517/dataset'\n",
    "data_path = 'C:/Users/Admin/Desktop/MIE1517_Project/data'\n",
    "drive_content = os.listdir(data_path)\n",
    "print(drive_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:40:11.623428127Z",
     "start_time": "2023-11-12T19:40:11.619135809Z"
    },
    "id": "lHFVdw5apSEu"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:40:11.663491795Z",
     "start_time": "2023-11-12T19:40:11.622755681Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T6_Ip_NB52CQ",
    "outputId": "c32d294c-3d28-4010-8340-c747d34b8a47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger',\n",
       " 'contempt',\n",
       " 'disgust',\n",
       " 'fear',\n",
       " 'happiness',\n",
       " 'neutrality',\n",
       " 'sadness',\n",
       " 'surprise']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classeses = []\n",
    "for i in os.listdir(data_path):\n",
    "    if i!=5:\n",
    "        classeses.append(i)\n",
    "classeses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:40:11.663905150Z",
     "start_time": "2023-11-12T19:40:11.663263045Z"
    },
    "id": "EkEP0L_ASYVb"
   },
   "outputs": [],
   "source": [
    "# def gauss_noise_tensor(img):\n",
    "#     assert isinstance(img, torch.Tensor)\n",
    "#     dtype = img.dtype\n",
    "#     if not img.is_floating_point():\n",
    "#         img = img.to(torch.float32)\n",
    "\n",
    "#     sigma = 25.0\n",
    "\n",
    "#     out = img + sigma * torch.randn_like(img)\n",
    "\n",
    "#     if out.dtype != dtype:\n",
    "#         out = out.to(dtype)\n",
    "\n",
    "#     return out\n",
    "# class ImgAugTransform:\n",
    "#     def __init__(self):\n",
    "#         self.aug = iaa.Sequential([\n",
    "#             iaa.Resize({\"height\": 224, \"width\": 224}),\n",
    "#             iaa.Sometimes(0.25, iaa.GaussianBlur(sigma=(0, 3.0))),\n",
    "#             iaa.Sometimes(0.25, iaa.Affine(rotate=(-20, 20), mode='symmetric')),\n",
    "#             iaa.Sometimes(0.25, iaa.OneOf([\n",
    "#                 iaa.Dropout(p=(0, 0.1)),\n",
    "#                 iaa.CoarseDropout(0.1, size_percent=0.5)\n",
    "#             ])),\n",
    "#             iaa.AddToHueAndSaturation(value=(-10, 10), per_channel=True)\n",
    "#         ])\n",
    "\n",
    "#     def __call__(self, img):\n",
    "#         img = np.array(img)\n",
    "\n",
    "#         augmented_img = self.aug.augment_image(img)\n",
    "\n",
    "#         # Convert the augmented image to PyTorch tensor\n",
    "#         augmented_tensor = F1.to_tensor(augmented_img)\n",
    "#         return self.aug.augment_image(img)\n",
    "#   imgaug_transform = ImgAugTransform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```\n",
    "Our dataset contains over 5000 photos. Training on this dataset requires significant computing power. However, the best hardware we have is a computer equipped with an Nvidia RTX2080 GPU. To ensure that the model can be trained on the entire dataset, we plan to use 5% of the original data as our training dataset. Below is our code for extracting a small training dataset, a small training function, and the CNN model structure.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:40:11.839983308Z",
     "start_time": "2023-11-12T19:40:11.663367478Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjPs44Ion4HI",
    "outputId": "fbcb4fd7-c826-43bb-c41f-22eaa10adc66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Label 0: 37 images\n",
      "Label 1: 10 images\n",
      "Label 2: 21 images\n",
      "Label 3: 26 images\n",
      "Label 4: 28 images\n",
      "Label 5: 70 images\n",
      "Label 6: 38 images\n",
      "Label 7: 44 images\n"
     ]
    }
   ],
   "source": [
    "features_or = []\n",
    "labels_or = []\n",
    "train_data_or = []\n",
    "data_transform1 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "def create_train2(data_percentage=5):\n",
    "    for i, photo in enumerate(classeses):\n",
    "        path = os.path.join(data_path, photo)\n",
    "        label = i  # Use the index of the class as the label\n",
    "\n",
    "        # List all images in the class folder\n",
    "        img_list = os.listdir(path)\n",
    "\n",
    "        # Calculate the number of images to load (5% of total)\n",
    "        num_images_to_load = int(len(img_list) * (data_percentage / 100))\n",
    "\n",
    "        # Randomly select a subset of images\n",
    "        selected_images = random.sample(img_list, num_images_to_load)\n",
    "\n",
    "\n",
    "        for img_name in selected_images:\n",
    "            img_path = os.path.join(path, img_name)\n",
    "            img_pil = Image.open(img_path)\n",
    "            transformed_image1 = data_transform1(img_pil)\n",
    "\n",
    "            if img_pil is None:\n",
    "                print(\"Error loading image:\", img_path)\n",
    "                continue\n",
    "\n",
    "            # Append the original image and its label to the list\n",
    "            features_or.append(transformed_image1)\n",
    "            labels_or.append(label)\n",
    "\n",
    "            train_data_or.append([img_pil, label])\n",
    "\n",
    "create_train2(data_percentage=5)  # Load only 5% of the data\n",
    "print(type(features_or[0]))\n",
    "# Count the occurrences of each label\n",
    "label_counts_or = Counter(labels_or)\n",
    "\n",
    "# Print the label counts\n",
    "for label, count in label_counts_or.items():\n",
    "    print(f\"Label {label}: {count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:40:11.897057207Z",
     "start_time": "2023-11-12T19:40:11.841400630Z"
    },
    "id": "46ITZDBRT51_"
   },
   "outputs": [],
   "source": [
    "# Define a transformation to convert PIL images to PyTorch tensors\n",
    "data_transform_to_tensor = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Convert images and labels to PyTorch tensors\n",
    "train_images = torch.stack([data_transform_to_tensor(data[0]) for data in train_data_or])\n",
    "train_labels = torch.tensor([data[1] for data in train_data_or])\n",
    "\n",
    "# Assuming you have a DataLoader for your training loop\n",
    "train_dataset = torch.utils.data.TensorDataset(train_images, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:40:11.939482143Z",
     "start_time": "2023-11-12T19:40:11.898803195Z"
    },
    "id": "bxh0XEA-SRrE"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(model, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for imgs, labels in torch.utils.data.DataLoader(dataset, batch_size=64):\n",
    "        #############################################\n",
    "        #To Enable GPU Usage\n",
    "        if use_cuda and torch.cuda.is_available():\n",
    "          imgs = imgs.cuda()\n",
    "          labels = labels.cuda()\n",
    "        #############################################\n",
    "        output = model(imgs)\n",
    "        #select index with maximum prediction score\n",
    "        pred = output.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "        total += imgs.shape[0]\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:40:11.939654343Z",
     "start_time": "2023-11-12T19:40:11.939390940Z"
    },
    "id": "FE_M9cyTQ9_l"
   },
   "outputs": [],
   "source": [
    "def trainSmall(model, data, batch_size, learning_rate, num_epochs):\n",
    "    train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_acc = []\n",
    "\n",
    "    # training\n",
    "    print(\"Training Start\")\n",
    "    n = 0  # the number of iterations\n",
    "    for epoch in range(num_epochs):\n",
    "        for imgs, labels in iter(train_loader):\n",
    "            #############################################\n",
    "            # To Enable GPU Usage\n",
    "            imgs = imgs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            #############################################\n",
    "\n",
    "            # Check if imgs is already a tensor\n",
    "            if not torch.is_tensor(imgs):\n",
    "                img_to_tensor = transforms.ToTensor()\n",
    "                imgs = img_to_tensor(imgs)\n",
    "\n",
    "                # Convert input to float\n",
    "                imgs = imgs.float()\n",
    "\n",
    "            # Convert the input tensor to the desired type\n",
    "            imgs = imgs.type(torch.float32)\n",
    "\n",
    "            out = model(imgs)  # forward pass\n",
    "            loss = criterion(out, labels)  # compute the total loss\n",
    "            loss.backward()  # backward pass (compute parameter updates)\n",
    "            optimizer.step()  # make the updates for each parameter\n",
    "            optimizer.zero_grad()  # a clean-up step for PyTorch\n",
    "            n += 1\n",
    "\n",
    "        train_accuracy = get_accuracy(model, data)\n",
    "        train_acc.append(train_accuracy)\n",
    "        print(\"Epoch:{}, Accuracy:{}\".format(epoch, train_accuracy))\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:40:11.939961348Z",
     "start_time": "2023-11-12T19:40:11.939564648Z"
    },
    "id": "ghFYeltCS2-l"
   },
   "outputs": [],
   "source": [
    "class CNNLargeNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNLargeNet2, self).__init__()\n",
    "        self.name = \"CNN\"\n",
    "\n",
    "        # Define the sequential layers for feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            # Convolutional Layer 1\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Convolutional Layer 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Convolutional Layer 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Convolutional Layer 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Convolutional Layer 5\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.Dropout(p=0.5),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 8)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # Pass the input through the feature layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for the classifier\n",
    "        x = self.classifier(x)  # Pass the flattened tensor through the classifier\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Below is the process by which we tuned the hyperparameters and trained the model to achieve overfitting. During this process we tried many different combinations of hyperparameters, and only a few examples are shown below along with their results. In the end, we reached overfitting after 260 epochs with batch_size = 64 and learning_rate = 0.005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T20:27:15.904408860Z",
     "start_time": "2023-11-12T20:21:55.027701485Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start\n",
      "Epoch:0, Accuracy:0.07664233576642336\n",
      "Epoch:1, Accuracy:0.2591240875912409\n",
      "Epoch:2, Accuracy:0.16058394160583941\n",
      "Epoch:3, Accuracy:0.1678832116788321\n",
      "Epoch:4, Accuracy:0.1678832116788321\n",
      "Epoch:5, Accuracy:0.16058394160583941\n",
      "Epoch:6, Accuracy:0.1678832116788321\n",
      "Epoch:7, Accuracy:0.17883211678832117\n",
      "Epoch:8, Accuracy:0.17883211678832117\n",
      "Epoch:9, Accuracy:0.18248175182481752\n",
      "Epoch:10, Accuracy:0.17883211678832117\n",
      "Epoch:11, Accuracy:0.17883211678832117\n",
      "Epoch:12, Accuracy:0.19708029197080293\n",
      "Epoch:13, Accuracy:0.18613138686131386\n",
      "Epoch:14, Accuracy:0.17883211678832117\n",
      "Epoch:15, Accuracy:0.1678832116788321\n",
      "Epoch:16, Accuracy:0.17153284671532848\n",
      "Epoch:17, Accuracy:0.19343065693430658\n",
      "Epoch:18, Accuracy:0.1897810218978102\n",
      "Epoch:19, Accuracy:0.18248175182481752\n",
      "Epoch:20, Accuracy:0.18613138686131386\n",
      "Epoch:21, Accuracy:0.20072992700729927\n",
      "Epoch:22, Accuracy:0.20437956204379562\n",
      "Epoch:23, Accuracy:0.20437956204379562\n",
      "Epoch:24, Accuracy:0.20802919708029197\n",
      "Epoch:25, Accuracy:0.21532846715328466\n",
      "Epoch:26, Accuracy:0.20802919708029197\n",
      "Epoch:27, Accuracy:0.21897810218978103\n",
      "Epoch:28, Accuracy:0.2116788321167883\n",
      "Epoch:29, Accuracy:0.20802919708029197\n",
      "Epoch:30, Accuracy:0.19708029197080293\n",
      "Epoch:31, Accuracy:0.2116788321167883\n",
      "Epoch:32, Accuracy:0.21532846715328466\n",
      "Epoch:33, Accuracy:0.21532846715328466\n",
      "Epoch:34, Accuracy:0.21897810218978103\n",
      "Epoch:35, Accuracy:0.22262773722627738\n",
      "Epoch:36, Accuracy:0.22627737226277372\n",
      "Epoch:37, Accuracy:0.23357664233576642\n",
      "Epoch:38, Accuracy:0.22992700729927007\n",
      "Epoch:39, Accuracy:0.23722627737226276\n",
      "Epoch:40, Accuracy:0.24087591240875914\n",
      "Epoch:41, Accuracy:0.24452554744525548\n",
      "Epoch:42, Accuracy:0.24452554744525548\n",
      "Epoch:43, Accuracy:0.23722627737226276\n",
      "Epoch:44, Accuracy:0.27007299270072993\n",
      "Epoch:45, Accuracy:0.22262773722627738\n",
      "Epoch:46, Accuracy:0.3175182481751825\n",
      "Epoch:47, Accuracy:0.22992700729927007\n",
      "Epoch:48, Accuracy:0.354014598540146\n",
      "Epoch:49, Accuracy:0.2518248175182482\n",
      "Epoch:50, Accuracy:0.3284671532846715\n",
      "Epoch:51, Accuracy:0.3102189781021898\n",
      "Epoch:52, Accuracy:0.3284671532846715\n",
      "Epoch:53, Accuracy:0.3357664233576642\n",
      "Epoch:54, Accuracy:0.34306569343065696\n",
      "Epoch:55, Accuracy:0.33941605839416056\n",
      "Epoch:56, Accuracy:0.35766423357664234\n",
      "Epoch:57, Accuracy:0.36496350364963503\n",
      "Epoch:58, Accuracy:0.40875912408759124\n",
      "Epoch:59, Accuracy:0.36496350364963503\n",
      "Epoch:60, Accuracy:0.40145985401459855\n",
      "Epoch:61, Accuracy:0.3686131386861314\n",
      "Epoch:62, Accuracy:0.4562043795620438\n",
      "Epoch:63, Accuracy:0.3795620437956204\n",
      "Epoch:64, Accuracy:0.4343065693430657\n",
      "Epoch:65, Accuracy:0.4781021897810219\n",
      "Epoch:66, Accuracy:0.32116788321167883\n",
      "Epoch:67, Accuracy:0.20072992700729927\n",
      "Epoch:68, Accuracy:0.18248175182481752\n",
      "Epoch:69, Accuracy:0.17518248175182483\n",
      "Epoch:70, Accuracy:0.18248175182481752\n",
      "Epoch:71, Accuracy:0.1678832116788321\n",
      "Epoch:72, Accuracy:0.15328467153284672\n",
      "Epoch:73, Accuracy:0.28102189781021897\n",
      "Epoch:74, Accuracy:0.28832116788321166\n",
      "Epoch:75, Accuracy:0.18613138686131386\n",
      "Epoch:76, Accuracy:0.18613138686131386\n",
      "Epoch:77, Accuracy:0.18613138686131386\n",
      "Epoch:78, Accuracy:0.18248175182481752\n",
      "Epoch:79, Accuracy:0.18613138686131386\n",
      "Epoch:80, Accuracy:0.18613138686131386\n",
      "Epoch:81, Accuracy:0.18613138686131386\n",
      "Epoch:82, Accuracy:0.18613138686131386\n",
      "Epoch:83, Accuracy:0.1897810218978102\n",
      "Epoch:84, Accuracy:0.18613138686131386\n",
      "Epoch:85, Accuracy:0.18613138686131386\n",
      "Epoch:86, Accuracy:0.18613138686131386\n",
      "Epoch:87, Accuracy:0.19343065693430658\n",
      "Epoch:88, Accuracy:0.1897810218978102\n",
      "Epoch:89, Accuracy:0.19343065693430658\n",
      "Epoch:90, Accuracy:0.19343065693430658\n",
      "Epoch:91, Accuracy:0.19343065693430658\n",
      "Epoch:92, Accuracy:0.19343065693430658\n",
      "Epoch:93, Accuracy:0.19343065693430658\n",
      "Epoch:94, Accuracy:0.1897810218978102\n",
      "Epoch:95, Accuracy:0.19343065693430658\n",
      "Epoch:96, Accuracy:0.19343065693430658\n",
      "Epoch:97, Accuracy:0.1897810218978102\n",
      "Epoch:98, Accuracy:0.1897810218978102\n",
      "Epoch:99, Accuracy:0.19343065693430658\n",
      "Epoch:100, Accuracy:0.19343065693430658\n",
      "Epoch:101, Accuracy:0.19343065693430658\n",
      "Epoch:102, Accuracy:0.19343065693430658\n",
      "Epoch:103, Accuracy:0.19343065693430658\n",
      "Epoch:104, Accuracy:0.19343065693430658\n",
      "Epoch:105, Accuracy:0.19343065693430658\n",
      "Epoch:106, Accuracy:0.18613138686131386\n",
      "Epoch:107, Accuracy:0.19343065693430658\n",
      "Epoch:108, Accuracy:0.19343065693430658\n",
      "Epoch:109, Accuracy:0.19343065693430658\n",
      "Epoch:110, Accuracy:0.1897810218978102\n",
      "Epoch:111, Accuracy:0.19343065693430658\n",
      "Epoch:112, Accuracy:0.19343065693430658\n",
      "Epoch:113, Accuracy:0.19343065693430658\n",
      "Epoch:114, Accuracy:0.1897810218978102\n",
      "Epoch:115, Accuracy:0.19343065693430658\n",
      "Epoch:116, Accuracy:0.19343065693430658\n",
      "Epoch:117, Accuracy:0.19343065693430658\n",
      "Epoch:118, Accuracy:0.19343065693430658\n",
      "Epoch:119, Accuracy:0.19343065693430658\n",
      "Epoch:120, Accuracy:0.19343065693430658\n",
      "Epoch:121, Accuracy:0.19343065693430658\n",
      "Epoch:122, Accuracy:0.19343065693430658\n",
      "Epoch:123, Accuracy:0.19343065693430658\n",
      "Epoch:124, Accuracy:0.19343065693430658\n",
      "Epoch:125, Accuracy:0.19343065693430658\n",
      "Epoch:126, Accuracy:0.19343065693430658\n",
      "Epoch:127, Accuracy:0.19343065693430658\n",
      "Epoch:128, Accuracy:0.19343065693430658\n",
      "Epoch:129, Accuracy:0.19343065693430658\n",
      "Epoch:130, Accuracy:0.19343065693430658\n",
      "Epoch:131, Accuracy:0.19343065693430658\n",
      "Epoch:132, Accuracy:0.19343065693430658\n",
      "Epoch:133, Accuracy:0.19343065693430658\n",
      "Epoch:134, Accuracy:0.19343065693430658\n",
      "Epoch:135, Accuracy:0.19343065693430658\n",
      "Epoch:136, Accuracy:0.19343065693430658\n",
      "Epoch:137, Accuracy:0.19343065693430658\n",
      "Epoch:138, Accuracy:0.19343065693430658\n",
      "Epoch:139, Accuracy:0.19343065693430658\n",
      "Epoch:140, Accuracy:0.19343065693430658\n",
      "Epoch:141, Accuracy:0.19343065693430658\n",
      "Epoch:142, Accuracy:0.19343065693430658\n",
      "Epoch:143, Accuracy:0.19343065693430658\n",
      "Epoch:144, Accuracy:0.19343065693430658\n",
      "Epoch:145, Accuracy:0.19343065693430658\n",
      "Epoch:146, Accuracy:0.19343065693430658\n",
      "Epoch:147, Accuracy:0.19343065693430658\n",
      "Epoch:148, Accuracy:0.19343065693430658\n",
      "Epoch:149, Accuracy:0.19343065693430658\n",
      "Epoch:150, Accuracy:0.19708029197080293\n",
      "Epoch:151, Accuracy:0.19708029197080293\n",
      "Epoch:152, Accuracy:0.19708029197080293\n",
      "Epoch:153, Accuracy:0.19343065693430658\n",
      "Epoch:154, Accuracy:0.19343065693430658\n",
      "Epoch:155, Accuracy:0.19343065693430658\n",
      "Epoch:156, Accuracy:0.19343065693430658\n",
      "Epoch:157, Accuracy:0.19343065693430658\n",
      "Epoch:158, Accuracy:0.19343065693430658\n",
      "Epoch:159, Accuracy:0.19343065693430658\n",
      "Epoch:160, Accuracy:0.19343065693430658\n",
      "Epoch:161, Accuracy:0.19343065693430658\n",
      "Epoch:162, Accuracy:0.19343065693430658\n",
      "Epoch:163, Accuracy:0.19343065693430658\n",
      "Epoch:164, Accuracy:0.19343065693430658\n",
      "Epoch:165, Accuracy:0.19343065693430658\n",
      "Epoch:166, Accuracy:0.19343065693430658\n",
      "Epoch:167, Accuracy:0.19343065693430658\n",
      "Epoch:168, Accuracy:0.19343065693430658\n",
      "Epoch:169, Accuracy:0.19343065693430658\n",
      "Epoch:170, Accuracy:0.19343065693430658\n",
      "Epoch:171, Accuracy:0.19343065693430658\n",
      "Epoch:172, Accuracy:0.19343065693430658\n",
      "Epoch:173, Accuracy:0.19343065693430658\n",
      "Epoch:174, Accuracy:0.19343065693430658\n",
      "Epoch:175, Accuracy:0.19343065693430658\n",
      "Epoch:176, Accuracy:0.19343065693430658\n",
      "Epoch:177, Accuracy:0.19343065693430658\n",
      "Epoch:178, Accuracy:0.19708029197080293\n",
      "Epoch:179, Accuracy:0.19343065693430658\n",
      "Epoch:180, Accuracy:0.19708029197080293\n",
      "Epoch:181, Accuracy:0.19708029197080293\n",
      "Epoch:182, Accuracy:0.19708029197080293\n",
      "Epoch:183, Accuracy:0.19708029197080293\n",
      "Epoch:184, Accuracy:0.1897810218978102\n",
      "Epoch:185, Accuracy:0.19708029197080293\n",
      "Epoch:186, Accuracy:0.19708029197080293\n",
      "Epoch:187, Accuracy:0.19708029197080293\n",
      "Epoch:188, Accuracy:0.19343065693430658\n",
      "Epoch:189, Accuracy:0.19343065693430658\n",
      "Epoch:190, Accuracy:0.19343065693430658\n",
      "Epoch:191, Accuracy:0.19343065693430658\n",
      "Epoch:192, Accuracy:0.19343065693430658\n",
      "Epoch:193, Accuracy:0.19343065693430658\n",
      "Epoch:194, Accuracy:0.19343065693430658\n",
      "Epoch:195, Accuracy:0.19343065693430658\n",
      "Epoch:196, Accuracy:0.19343065693430658\n",
      "Epoch:197, Accuracy:0.19343065693430658\n",
      "Epoch:198, Accuracy:0.19343065693430658\n",
      "Epoch:199, Accuracy:0.19343065693430658\n",
      "Epoch:200, Accuracy:0.19708029197080293\n",
      "Epoch:201, Accuracy:0.19708029197080293\n",
      "Epoch:202, Accuracy:0.19708029197080293\n",
      "Epoch:203, Accuracy:0.19708029197080293\n",
      "Epoch:204, Accuracy:0.19708029197080293\n",
      "Epoch:205, Accuracy:0.19708029197080293\n",
      "Epoch:206, Accuracy:0.19708029197080293\n",
      "Epoch:207, Accuracy:0.19708029197080293\n",
      "Epoch:208, Accuracy:0.19708029197080293\n",
      "Epoch:209, Accuracy:0.19708029197080293\n",
      "Epoch:210, Accuracy:0.18613138686131386\n",
      "Epoch:211, Accuracy:0.19708029197080293\n",
      "Epoch:212, Accuracy:0.19708029197080293\n",
      "Epoch:213, Accuracy:0.19343065693430658\n",
      "Epoch:214, Accuracy:0.19343065693430658\n",
      "Epoch:215, Accuracy:0.19708029197080293\n",
      "Epoch:216, Accuracy:0.20072992700729927\n",
      "Epoch:217, Accuracy:0.1897810218978102\n",
      "Epoch:218, Accuracy:0.1897810218978102\n",
      "Epoch:219, Accuracy:0.17883211678832117\n",
      "Epoch:220, Accuracy:0.16058394160583941\n",
      "Epoch:221, Accuracy:0.16058394160583941\n",
      "Epoch:222, Accuracy:0.16423357664233576\n",
      "Epoch:223, Accuracy:0.16058394160583941\n",
      "Epoch:224, Accuracy:0.16423357664233576\n",
      "Epoch:225, Accuracy:0.16423357664233576\n",
      "Epoch:226, Accuracy:0.16423357664233576\n",
      "Epoch:227, Accuracy:0.16058394160583941\n",
      "Epoch:228, Accuracy:0.16058394160583941\n",
      "Epoch:229, Accuracy:0.16058394160583941\n",
      "Epoch:230, Accuracy:0.16423357664233576\n",
      "Epoch:231, Accuracy:0.16423357664233576\n",
      "Epoch:232, Accuracy:0.16058394160583941\n",
      "Epoch:233, Accuracy:0.16058394160583941\n",
      "Epoch:234, Accuracy:0.16423357664233576\n",
      "Epoch:235, Accuracy:0.16423357664233576\n",
      "Epoch:236, Accuracy:0.16423357664233576\n",
      "Epoch:237, Accuracy:0.16423357664233576\n",
      "Epoch:238, Accuracy:0.16423357664233576\n",
      "Epoch:239, Accuracy:0.16423357664233576\n",
      "Epoch:240, Accuracy:0.16423357664233576\n",
      "Epoch:241, Accuracy:0.16423357664233576\n",
      "Epoch:242, Accuracy:0.16423357664233576\n",
      "Epoch:243, Accuracy:0.16058394160583941\n",
      "Epoch:244, Accuracy:0.16423357664233576\n",
      "Epoch:245, Accuracy:0.16423357664233576\n",
      "Epoch:246, Accuracy:0.16058394160583941\n",
      "Epoch:247, Accuracy:0.16058394160583941\n",
      "Epoch:248, Accuracy:0.16058394160583941\n",
      "Epoch:249, Accuracy:0.16058394160583941\n",
      "Epoch:250, Accuracy:0.16058394160583941\n",
      "Epoch:251, Accuracy:0.16058394160583941\n",
      "Epoch:252, Accuracy:0.16058394160583941\n",
      "Epoch:253, Accuracy:0.16058394160583941\n",
      "Epoch:254, Accuracy:0.16058394160583941\n",
      "Epoch:255, Accuracy:0.16058394160583941\n",
      "Epoch:256, Accuracy:0.16058394160583941\n",
      "Epoch:257, Accuracy:0.16058394160583941\n",
      "Epoch:258, Accuracy:0.16058394160583941\n",
      "Epoch:259, Accuracy:0.16058394160583941\n",
      "Epoch:260, Accuracy:0.16058394160583941\n",
      "Epoch:261, Accuracy:0.16058394160583941\n",
      "Epoch:262, Accuracy:0.16058394160583941\n",
      "Epoch:263, Accuracy:0.16058394160583941\n",
      "Epoch:264, Accuracy:0.16058394160583941\n",
      "Epoch:265, Accuracy:0.16058394160583941\n",
      "Epoch:266, Accuracy:0.16058394160583941\n",
      "Epoch:267, Accuracy:0.16058394160583941\n",
      "Epoch:268, Accuracy:0.16058394160583941\n",
      "Epoch:269, Accuracy:0.16058394160583941\n",
      "Epoch:270, Accuracy:0.16058394160583941\n",
      "Epoch:271, Accuracy:0.16058394160583941\n",
      "Epoch:272, Accuracy:0.16058394160583941\n",
      "Epoch:273, Accuracy:0.16058394160583941\n",
      "Epoch:274, Accuracy:0.16058394160583941\n",
      "Epoch:275, Accuracy:0.16058394160583941\n",
      "Epoch:276, Accuracy:0.16058394160583941\n",
      "Epoch:277, Accuracy:0.16058394160583941\n",
      "Epoch:278, Accuracy:0.16058394160583941\n",
      "Epoch:279, Accuracy:0.16058394160583941\n",
      "Epoch:280, Accuracy:0.16058394160583941\n",
      "Epoch:281, Accuracy:0.16058394160583941\n",
      "Epoch:282, Accuracy:0.16058394160583941\n",
      "Epoch:283, Accuracy:0.16058394160583941\n",
      "Epoch:284, Accuracy:0.16058394160583941\n",
      "Epoch:285, Accuracy:0.16058394160583941\n",
      "Epoch:286, Accuracy:0.16058394160583941\n",
      "Epoch:287, Accuracy:0.16058394160583941\n",
      "Epoch:288, Accuracy:0.16058394160583941\n",
      "Epoch:289, Accuracy:0.16058394160583941\n",
      "Epoch:290, Accuracy:0.16058394160583941\n",
      "Epoch:291, Accuracy:0.16058394160583941\n",
      "Epoch:292, Accuracy:0.16058394160583941\n",
      "Epoch:293, Accuracy:0.16058394160583941\n",
      "Epoch:294, Accuracy:0.16058394160583941\n",
      "Epoch:295, Accuracy:0.16058394160583941\n",
      "Epoch:296, Accuracy:0.16058394160583941\n",
      "Epoch:297, Accuracy:0.16058394160583941\n",
      "Epoch:298, Accuracy:0.16058394160583941\n",
      "Epoch:299, Accuracy:0.16058394160583941\n",
      "Epoch:300, Accuracy:0.16058394160583941\n",
      "Epoch:301, Accuracy:0.16058394160583941\n",
      "Epoch:302, Accuracy:0.16058394160583941\n",
      "Epoch:303, Accuracy:0.16058394160583941\n",
      "Epoch:304, Accuracy:0.16058394160583941\n",
      "Epoch:305, Accuracy:0.16058394160583941\n",
      "Epoch:306, Accuracy:0.16058394160583941\n",
      "Epoch:307, Accuracy:0.16058394160583941\n",
      "Epoch:308, Accuracy:0.16058394160583941\n",
      "Epoch:309, Accuracy:0.16058394160583941\n",
      "Epoch:310, Accuracy:0.16058394160583941\n",
      "Epoch:311, Accuracy:0.16058394160583941\n",
      "Epoch:312, Accuracy:0.16058394160583941\n",
      "Epoch:313, Accuracy:0.16058394160583941\n",
      "Epoch:314, Accuracy:0.16058394160583941\n",
      "Epoch:315, Accuracy:0.16058394160583941\n",
      "Epoch:316, Accuracy:0.16058394160583941\n",
      "Epoch:317, Accuracy:0.16058394160583941\n",
      "Epoch:318, Accuracy:0.16058394160583941\n",
      "Epoch:319, Accuracy:0.16058394160583941\n",
      "Epoch:320, Accuracy:0.16058394160583941\n",
      "Epoch:321, Accuracy:0.16058394160583941\n",
      "Epoch:322, Accuracy:0.16058394160583941\n",
      "Epoch:323, Accuracy:0.16058394160583941\n",
      "Epoch:324, Accuracy:0.16058394160583941\n",
      "Epoch:325, Accuracy:0.16058394160583941\n",
      "Epoch:326, Accuracy:0.16058394160583941\n",
      "Epoch:327, Accuracy:0.16058394160583941\n",
      "Epoch:328, Accuracy:0.16058394160583941\n",
      "Epoch:329, Accuracy:0.16058394160583941\n",
      "Epoch:330, Accuracy:0.16058394160583941\n",
      "Epoch:331, Accuracy:0.16058394160583941\n",
      "Epoch:332, Accuracy:0.16058394160583941\n",
      "Epoch:333, Accuracy:0.16058394160583941\n",
      "Epoch:334, Accuracy:0.16058394160583941\n",
      "Epoch:335, Accuracy:0.16058394160583941\n",
      "Epoch:336, Accuracy:0.16058394160583941\n",
      "Epoch:337, Accuracy:0.16058394160583941\n",
      "Epoch:338, Accuracy:0.16058394160583941\n",
      "Epoch:339, Accuracy:0.16058394160583941\n",
      "Epoch:340, Accuracy:0.16058394160583941\n",
      "Epoch:341, Accuracy:0.16058394160583941\n",
      "Epoch:342, Accuracy:0.16058394160583941\n",
      "Epoch:343, Accuracy:0.16058394160583941\n",
      "Epoch:344, Accuracy:0.16058394160583941\n",
      "Epoch:345, Accuracy:0.16058394160583941\n",
      "Epoch:346, Accuracy:0.16058394160583941\n",
      "Epoch:347, Accuracy:0.16058394160583941\n",
      "Epoch:348, Accuracy:0.16058394160583941\n",
      "Epoch:349, Accuracy:0.16058394160583941\n",
      "Epoch:350, Accuracy:0.16058394160583941\n",
      "Epoch:351, Accuracy:0.16058394160583941\n",
      "Epoch:352, Accuracy:0.16058394160583941\n",
      "Epoch:353, Accuracy:0.16058394160583941\n",
      "Epoch:354, Accuracy:0.16058394160583941\n",
      "Epoch:355, Accuracy:0.16058394160583941\n",
      "Epoch:356, Accuracy:0.16058394160583941\n",
      "Epoch:357, Accuracy:0.16058394160583941\n",
      "Epoch:358, Accuracy:0.16058394160583941\n",
      "Epoch:359, Accuracy:0.16058394160583941\n",
      "Epoch:360, Accuracy:0.16058394160583941\n",
      "Epoch:361, Accuracy:0.16058394160583941\n",
      "Epoch:362, Accuracy:0.16058394160583941\n",
      "Epoch:363, Accuracy:0.16058394160583941\n",
      "Epoch:364, Accuracy:0.16058394160583941\n",
      "Epoch:365, Accuracy:0.16058394160583941\n",
      "Epoch:366, Accuracy:0.16058394160583941\n",
      "Epoch:367, Accuracy:0.16058394160583941\n",
      "Epoch:368, Accuracy:0.16058394160583941\n",
      "Epoch:369, Accuracy:0.16058394160583941\n",
      "Epoch:370, Accuracy:0.16058394160583941\n",
      "Epoch:371, Accuracy:0.16058394160583941\n",
      "Epoch:372, Accuracy:0.16058394160583941\n",
      "Epoch:373, Accuracy:0.16058394160583941\n",
      "Epoch:374, Accuracy:0.16058394160583941\n",
      "Epoch:375, Accuracy:0.16058394160583941\n",
      "Epoch:376, Accuracy:0.16058394160583941\n",
      "Epoch:377, Accuracy:0.16058394160583941\n",
      "Epoch:378, Accuracy:0.16058394160583941\n",
      "Epoch:379, Accuracy:0.16058394160583941\n",
      "Epoch:380, Accuracy:0.16058394160583941\n",
      "Epoch:381, Accuracy:0.16058394160583941\n",
      "Epoch:382, Accuracy:0.16058394160583941\n",
      "Epoch:383, Accuracy:0.16058394160583941\n",
      "Epoch:384, Accuracy:0.16058394160583941\n",
      "Epoch:385, Accuracy:0.16058394160583941\n",
      "Epoch:386, Accuracy:0.16058394160583941\n",
      "Epoch:387, Accuracy:0.16058394160583941\n",
      "Epoch:388, Accuracy:0.16058394160583941\n",
      "Epoch:389, Accuracy:0.16058394160583941\n",
      "Epoch:390, Accuracy:0.16058394160583941\n",
      "Epoch:391, Accuracy:0.16058394160583941\n",
      "Epoch:392, Accuracy:0.16058394160583941\n",
      "Epoch:393, Accuracy:0.16058394160583941\n",
      "Epoch:394, Accuracy:0.16058394160583941\n",
      "Epoch:395, Accuracy:0.16058394160583941\n",
      "Epoch:396, Accuracy:0.16058394160583941\n",
      "Epoch:397, Accuracy:0.16058394160583941\n",
      "Epoch:398, Accuracy:0.16058394160583941\n",
      "Epoch:399, Accuracy:0.16058394160583941\n",
      "Epoch:400, Accuracy:0.16058394160583941\n",
      "Epoch:401, Accuracy:0.16058394160583941\n",
      "Epoch:402, Accuracy:0.16058394160583941\n",
      "Epoch:403, Accuracy:0.16058394160583941\n",
      "Epoch:404, Accuracy:0.16058394160583941\n",
      "Epoch:405, Accuracy:0.16058394160583941\n",
      "Epoch:406, Accuracy:0.16058394160583941\n",
      "Epoch:407, Accuracy:0.16058394160583941\n",
      "Epoch:408, Accuracy:0.16058394160583941\n",
      "Epoch:409, Accuracy:0.16058394160583941\n",
      "Epoch:410, Accuracy:0.16058394160583941\n",
      "Epoch:411, Accuracy:0.16058394160583941\n",
      "Epoch:412, Accuracy:0.16058394160583941\n",
      "Epoch:413, Accuracy:0.16058394160583941\n",
      "Epoch:414, Accuracy:0.16058394160583941\n",
      "Epoch:415, Accuracy:0.16058394160583941\n",
      "Epoch:416, Accuracy:0.16058394160583941\n",
      "Epoch:417, Accuracy:0.16058394160583941\n",
      "Epoch:418, Accuracy:0.16058394160583941\n",
      "Epoch:419, Accuracy:0.16058394160583941\n",
      "Epoch:420, Accuracy:0.16058394160583941\n",
      "Epoch:421, Accuracy:0.16058394160583941\n",
      "Epoch:422, Accuracy:0.16058394160583941\n",
      "Epoch:423, Accuracy:0.16058394160583941\n",
      "Epoch:424, Accuracy:0.16058394160583941\n",
      "Epoch:425, Accuracy:0.16058394160583941\n",
      "Epoch:426, Accuracy:0.16058394160583941\n",
      "Epoch:427, Accuracy:0.16058394160583941\n",
      "Epoch:428, Accuracy:0.16058394160583941\n",
      "Epoch:429, Accuracy:0.16058394160583941\n",
      "Epoch:430, Accuracy:0.16058394160583941\n",
      "Epoch:431, Accuracy:0.16058394160583941\n",
      "Epoch:432, Accuracy:0.16058394160583941\n",
      "Epoch:433, Accuracy:0.16058394160583941\n",
      "Epoch:434, Accuracy:0.16058394160583941\n",
      "Epoch:435, Accuracy:0.16058394160583941\n",
      "Epoch:436, Accuracy:0.16058394160583941\n",
      "Epoch:437, Accuracy:0.16058394160583941\n",
      "Epoch:438, Accuracy:0.16058394160583941\n",
      "Epoch:439, Accuracy:0.16058394160583941\n",
      "Epoch:440, Accuracy:0.16058394160583941\n",
      "Epoch:441, Accuracy:0.16058394160583941\n",
      "Epoch:442, Accuracy:0.16058394160583941\n",
      "Epoch:443, Accuracy:0.16058394160583941\n",
      "Epoch:444, Accuracy:0.16058394160583941\n",
      "Epoch:445, Accuracy:0.16058394160583941\n",
      "Epoch:446, Accuracy:0.16058394160583941\n",
      "Epoch:447, Accuracy:0.16058394160583941\n",
      "Epoch:448, Accuracy:0.16058394160583941\n",
      "Epoch:449, Accuracy:0.16058394160583941\n",
      "Epoch:450, Accuracy:0.16058394160583941\n",
      "Epoch:451, Accuracy:0.16058394160583941\n",
      "Epoch:452, Accuracy:0.16058394160583941\n",
      "Epoch:453, Accuracy:0.16058394160583941\n",
      "Epoch:454, Accuracy:0.16058394160583941\n",
      "Epoch:455, Accuracy:0.16058394160583941\n",
      "Epoch:456, Accuracy:0.16058394160583941\n",
      "Epoch:457, Accuracy:0.16058394160583941\n",
      "Epoch:458, Accuracy:0.16058394160583941\n",
      "Epoch:459, Accuracy:0.16058394160583941\n",
      "Epoch:460, Accuracy:0.16058394160583941\n",
      "Epoch:461, Accuracy:0.16058394160583941\n",
      "Epoch:462, Accuracy:0.16058394160583941\n",
      "Epoch:463, Accuracy:0.16058394160583941\n",
      "Epoch:464, Accuracy:0.16058394160583941\n",
      "Epoch:465, Accuracy:0.16058394160583941\n",
      "Epoch:466, Accuracy:0.16058394160583941\n",
      "Epoch:467, Accuracy:0.16058394160583941\n",
      "Epoch:468, Accuracy:0.16058394160583941\n",
      "Epoch:469, Accuracy:0.16058394160583941\n",
      "Epoch:470, Accuracy:0.16058394160583941\n",
      "Epoch:471, Accuracy:0.16058394160583941\n",
      "Epoch:472, Accuracy:0.16058394160583941\n",
      "Epoch:473, Accuracy:0.16058394160583941\n",
      "Epoch:474, Accuracy:0.16058394160583941\n",
      "Epoch:475, Accuracy:0.16058394160583941\n",
      "Epoch:476, Accuracy:0.16058394160583941\n",
      "Epoch:477, Accuracy:0.16058394160583941\n",
      "Epoch:478, Accuracy:0.16058394160583941\n",
      "Epoch:479, Accuracy:0.16058394160583941\n",
      "Epoch:480, Accuracy:0.16058394160583941\n",
      "Epoch:481, Accuracy:0.16058394160583941\n",
      "Epoch:482, Accuracy:0.16058394160583941\n",
      "Epoch:483, Accuracy:0.16058394160583941\n",
      "Epoch:484, Accuracy:0.16058394160583941\n",
      "Epoch:485, Accuracy:0.16058394160583941\n",
      "Epoch:486, Accuracy:0.16058394160583941\n",
      "Epoch:487, Accuracy:0.16058394160583941\n",
      "Epoch:488, Accuracy:0.16058394160583941\n",
      "Epoch:489, Accuracy:0.16058394160583941\n",
      "Epoch:490, Accuracy:0.16058394160583941\n",
      "Epoch:491, Accuracy:0.16058394160583941\n",
      "Epoch:492, Accuracy:0.16058394160583941\n",
      "Epoch:493, Accuracy:0.16058394160583941\n",
      "Epoch:494, Accuracy:0.16058394160583941\n",
      "Epoch:495, Accuracy:0.16058394160583941\n",
      "Epoch:496, Accuracy:0.16058394160583941\n",
      "Epoch:497, Accuracy:0.16058394160583941\n",
      "Epoch:498, Accuracy:0.16058394160583941\n",
      "Epoch:499, Accuracy:0.16058394160583941\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "if torch.cuda.is_available():\n",
    "    model = CNNLargeNet2().cuda()\n",
    "Y = trainSmall(model, train_dataset, batch_size=128, learning_rate=0.01, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:46:09.041882034Z",
     "start_time": "2023-11-12T19:40:11.939633793Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRPxddC5TKOk",
    "outputId": "36b893f9-dfd9-412c-939e-1cb163f31fec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start\n",
      "Epoch:0, Accuracy:0.16058394160583941\n",
      "Epoch:1, Accuracy:0.0948905109489051\n",
      "Epoch:2, Accuracy:0.16058394160583941\n",
      "Epoch:3, Accuracy:0.16423357664233576\n",
      "Epoch:4, Accuracy:0.15328467153284672\n",
      "Epoch:5, Accuracy:0.18613138686131386\n",
      "Epoch:6, Accuracy:0.1897810218978102\n",
      "Epoch:7, Accuracy:0.18248175182481752\n",
      "Epoch:8, Accuracy:0.17883211678832117\n",
      "Epoch:9, Accuracy:0.17883211678832117\n",
      "Epoch:10, Accuracy:0.18613138686131386\n",
      "Epoch:11, Accuracy:0.18613138686131386\n",
      "Epoch:12, Accuracy:0.18613138686131386\n",
      "Epoch:13, Accuracy:0.20072992700729927\n",
      "Epoch:14, Accuracy:0.20072992700729927\n",
      "Epoch:15, Accuracy:0.19343065693430658\n",
      "Epoch:16, Accuracy:0.20437956204379562\n",
      "Epoch:17, Accuracy:0.21532846715328466\n",
      "Epoch:18, Accuracy:0.2116788321167883\n",
      "Epoch:19, Accuracy:0.20802919708029197\n",
      "Epoch:20, Accuracy:0.21897810218978103\n",
      "Epoch:21, Accuracy:0.20072992700729927\n",
      "Epoch:22, Accuracy:0.22627737226277372\n",
      "Epoch:23, Accuracy:0.30656934306569344\n",
      "Epoch:24, Accuracy:0.2664233576642336\n",
      "Epoch:25, Accuracy:0.33941605839416056\n",
      "Epoch:26, Accuracy:0.3759124087591241\n",
      "Epoch:27, Accuracy:0.3795620437956204\n",
      "Epoch:28, Accuracy:0.41605839416058393\n",
      "Epoch:29, Accuracy:0.42700729927007297\n",
      "Epoch:30, Accuracy:0.45985401459854014\n",
      "Epoch:31, Accuracy:0.42700729927007297\n",
      "Epoch:32, Accuracy:0.20437956204379562\n",
      "Epoch:33, Accuracy:0.21532846715328466\n",
      "Epoch:34, Accuracy:0.24817518248175183\n",
      "Epoch:35, Accuracy:0.25547445255474455\n",
      "Epoch:36, Accuracy:0.25547445255474455\n",
      "Epoch:37, Accuracy:0.26277372262773724\n",
      "Epoch:38, Accuracy:0.2664233576642336\n",
      "Epoch:39, Accuracy:0.2664233576642336\n",
      "Epoch:40, Accuracy:0.24452554744525548\n",
      "Epoch:41, Accuracy:0.22262773722627738\n",
      "Epoch:42, Accuracy:0.23722627737226276\n",
      "Epoch:43, Accuracy:0.17153284671532848\n",
      "Epoch:44, Accuracy:0.1897810218978102\n",
      "Epoch:45, Accuracy:0.21532846715328466\n",
      "Epoch:46, Accuracy:0.20072992700729927\n",
      "Epoch:47, Accuracy:0.21897810218978103\n",
      "Epoch:48, Accuracy:0.20072992700729927\n",
      "Epoch:49, Accuracy:0.20437956204379562\n",
      "Epoch:50, Accuracy:0.2116788321167883\n",
      "Epoch:51, Accuracy:0.24087591240875914\n",
      "Epoch:52, Accuracy:0.21897810218978103\n",
      "Epoch:53, Accuracy:0.21532846715328466\n",
      "Epoch:54, Accuracy:0.20802919708029197\n",
      "Epoch:55, Accuracy:0.21897810218978103\n",
      "Epoch:56, Accuracy:0.22262773722627738\n",
      "Epoch:57, Accuracy:0.20072992700729927\n",
      "Epoch:58, Accuracy:0.19708029197080293\n",
      "Epoch:59, Accuracy:0.21897810218978103\n",
      "Epoch:60, Accuracy:0.21897810218978103\n",
      "Epoch:61, Accuracy:0.20072992700729927\n",
      "Epoch:62, Accuracy:0.24817518248175183\n",
      "Epoch:63, Accuracy:0.23722627737226276\n",
      "Epoch:64, Accuracy:0.22627737226277372\n",
      "Epoch:65, Accuracy:0.21532846715328466\n",
      "Epoch:66, Accuracy:0.20437956204379562\n",
      "Epoch:67, Accuracy:0.21532846715328466\n",
      "Epoch:68, Accuracy:0.23722627737226276\n",
      "Epoch:69, Accuracy:0.24817518248175183\n",
      "Epoch:70, Accuracy:0.21897810218978103\n",
      "Epoch:71, Accuracy:0.24087591240875914\n",
      "Epoch:72, Accuracy:0.2591240875912409\n",
      "Epoch:73, Accuracy:0.22262773722627738\n",
      "Epoch:74, Accuracy:0.20072992700729927\n",
      "Epoch:75, Accuracy:0.22627737226277372\n",
      "Epoch:76, Accuracy:0.2116788321167883\n",
      "Epoch:77, Accuracy:0.19708029197080293\n",
      "Epoch:78, Accuracy:0.2116788321167883\n",
      "Epoch:79, Accuracy:0.20072992700729927\n",
      "Epoch:80, Accuracy:0.21532846715328466\n",
      "Epoch:81, Accuracy:0.19708029197080293\n",
      "Epoch:82, Accuracy:0.2116788321167883\n",
      "Epoch:83, Accuracy:0.24087591240875914\n",
      "Epoch:84, Accuracy:0.24087591240875914\n",
      "Epoch:85, Accuracy:0.22992700729927007\n",
      "Epoch:86, Accuracy:0.21532846715328466\n",
      "Epoch:87, Accuracy:0.19343065693430658\n",
      "Epoch:88, Accuracy:0.21897810218978103\n",
      "Epoch:89, Accuracy:0.20437956204379562\n",
      "Epoch:90, Accuracy:0.20802919708029197\n",
      "Epoch:91, Accuracy:0.22992700729927007\n",
      "Epoch:92, Accuracy:0.22992700729927007\n",
      "Epoch:93, Accuracy:0.21897810218978103\n",
      "Epoch:94, Accuracy:0.20802919708029197\n",
      "Epoch:95, Accuracy:0.21532846715328466\n",
      "Epoch:96, Accuracy:0.21897810218978103\n",
      "Epoch:97, Accuracy:0.20072992700729927\n",
      "Epoch:98, Accuracy:0.22262773722627738\n",
      "Epoch:99, Accuracy:0.22627737226277372\n",
      "Epoch:100, Accuracy:0.22992700729927007\n",
      "Epoch:101, Accuracy:0.22262773722627738\n",
      "Epoch:102, Accuracy:0.20802919708029197\n",
      "Epoch:103, Accuracy:0.21532846715328466\n",
      "Epoch:104, Accuracy:0.22627737226277372\n",
      "Epoch:105, Accuracy:0.23357664233576642\n",
      "Epoch:106, Accuracy:0.24087591240875914\n",
      "Epoch:107, Accuracy:0.21532846715328466\n",
      "Epoch:108, Accuracy:0.22262773722627738\n",
      "Epoch:109, Accuracy:0.2116788321167883\n",
      "Epoch:110, Accuracy:0.20437956204379562\n",
      "Epoch:111, Accuracy:0.20072992700729927\n",
      "Epoch:112, Accuracy:0.2116788321167883\n",
      "Epoch:113, Accuracy:0.20802919708029197\n",
      "Epoch:114, Accuracy:0.22262773722627738\n",
      "Epoch:115, Accuracy:0.21897810218978103\n",
      "Epoch:116, Accuracy:0.2116788321167883\n",
      "Epoch:117, Accuracy:0.2116788321167883\n",
      "Epoch:118, Accuracy:0.21532846715328466\n",
      "Epoch:119, Accuracy:0.21897810218978103\n",
      "Epoch:120, Accuracy:0.21897810218978103\n",
      "Epoch:121, Accuracy:0.21897810218978103\n",
      "Epoch:122, Accuracy:0.22992700729927007\n",
      "Epoch:123, Accuracy:0.20802919708029197\n",
      "Epoch:124, Accuracy:0.21532846715328466\n",
      "Epoch:125, Accuracy:0.24087591240875914\n",
      "Epoch:126, Accuracy:0.24817518248175183\n",
      "Epoch:127, Accuracy:0.23722627737226276\n",
      "Epoch:128, Accuracy:0.22262773722627738\n",
      "Epoch:129, Accuracy:0.2116788321167883\n",
      "Epoch:130, Accuracy:0.22262773722627738\n",
      "Epoch:131, Accuracy:0.21897810218978103\n",
      "Epoch:132, Accuracy:0.23722627737226276\n",
      "Epoch:133, Accuracy:0.22992700729927007\n",
      "Epoch:134, Accuracy:0.23357664233576642\n",
      "Epoch:135, Accuracy:0.22992700729927007\n",
      "Epoch:136, Accuracy:0.23357664233576642\n",
      "Epoch:137, Accuracy:0.23722627737226276\n",
      "Epoch:138, Accuracy:0.23357664233576642\n",
      "Epoch:139, Accuracy:0.21897810218978103\n",
      "Epoch:140, Accuracy:0.23357664233576642\n",
      "Epoch:141, Accuracy:0.23722627737226276\n",
      "Epoch:142, Accuracy:0.24452554744525548\n",
      "Epoch:143, Accuracy:0.24087591240875914\n",
      "Epoch:144, Accuracy:0.2591240875912409\n",
      "Epoch:145, Accuracy:0.26277372262773724\n",
      "Epoch:146, Accuracy:0.24817518248175183\n",
      "Epoch:147, Accuracy:0.2591240875912409\n",
      "Epoch:148, Accuracy:0.24817518248175183\n",
      "Epoch:149, Accuracy:0.25547445255474455\n",
      "Epoch:150, Accuracy:0.2518248175182482\n",
      "Epoch:151, Accuracy:0.24452554744525548\n",
      "Epoch:152, Accuracy:0.26277372262773724\n",
      "Epoch:153, Accuracy:0.2773722627737226\n",
      "Epoch:154, Accuracy:0.24817518248175183\n",
      "Epoch:155, Accuracy:0.2591240875912409\n",
      "Epoch:156, Accuracy:0.27007299270072993\n",
      "Epoch:157, Accuracy:0.2773722627737226\n",
      "Epoch:158, Accuracy:0.29927007299270075\n",
      "Epoch:159, Accuracy:0.28102189781021897\n",
      "Epoch:160, Accuracy:0.28102189781021897\n",
      "Epoch:161, Accuracy:0.2737226277372263\n",
      "Epoch:162, Accuracy:0.27007299270072993\n",
      "Epoch:163, Accuracy:0.28102189781021897\n",
      "Epoch:164, Accuracy:0.2737226277372263\n",
      "Epoch:165, Accuracy:0.2773722627737226\n",
      "Epoch:166, Accuracy:0.26277372262773724\n",
      "Epoch:167, Accuracy:0.2664233576642336\n",
      "Epoch:168, Accuracy:0.2846715328467153\n",
      "Epoch:169, Accuracy:0.2846715328467153\n",
      "Epoch:170, Accuracy:0.28832116788321166\n",
      "Epoch:171, Accuracy:0.28102189781021897\n",
      "Epoch:172, Accuracy:0.28102189781021897\n",
      "Epoch:173, Accuracy:0.2956204379562044\n",
      "Epoch:174, Accuracy:0.2846715328467153\n",
      "Epoch:175, Accuracy:0.28102189781021897\n",
      "Epoch:176, Accuracy:0.3029197080291971\n",
      "Epoch:177, Accuracy:0.2956204379562044\n",
      "Epoch:178, Accuracy:0.28832116788321166\n",
      "Epoch:179, Accuracy:0.28832116788321166\n",
      "Epoch:180, Accuracy:0.3175182481751825\n",
      "Epoch:181, Accuracy:0.28832116788321166\n",
      "Epoch:182, Accuracy:0.3029197080291971\n",
      "Epoch:183, Accuracy:0.29927007299270075\n",
      "Epoch:184, Accuracy:0.2956204379562044\n",
      "Epoch:185, Accuracy:0.2956204379562044\n",
      "Epoch:186, Accuracy:0.29927007299270075\n",
      "Epoch:187, Accuracy:0.29927007299270075\n",
      "Epoch:188, Accuracy:0.3175182481751825\n",
      "Epoch:189, Accuracy:0.3102189781021898\n",
      "Epoch:190, Accuracy:0.3102189781021898\n",
      "Epoch:191, Accuracy:0.3248175182481752\n",
      "Epoch:192, Accuracy:0.31386861313868614\n",
      "Epoch:193, Accuracy:0.3175182481751825\n",
      "Epoch:194, Accuracy:0.3248175182481752\n",
      "Epoch:195, Accuracy:0.3248175182481752\n",
      "Epoch:196, Accuracy:0.291970802919708\n",
      "Epoch:197, Accuracy:0.3284671532846715\n",
      "Epoch:198, Accuracy:0.3284671532846715\n",
      "Epoch:199, Accuracy:0.31386861313868614\n",
      "Epoch:200, Accuracy:0.3175182481751825\n",
      "Epoch:201, Accuracy:0.3284671532846715\n",
      "Epoch:202, Accuracy:0.3248175182481752\n",
      "Epoch:203, Accuracy:0.33211678832116787\n",
      "Epoch:204, Accuracy:0.31386861313868614\n",
      "Epoch:205, Accuracy:0.3284671532846715\n",
      "Epoch:206, Accuracy:0.3284671532846715\n",
      "Epoch:207, Accuracy:0.3284671532846715\n",
      "Epoch:208, Accuracy:0.3284671532846715\n",
      "Epoch:209, Accuracy:0.33211678832116787\n",
      "Epoch:210, Accuracy:0.3357664233576642\n",
      "Epoch:211, Accuracy:0.33211678832116787\n",
      "Epoch:212, Accuracy:0.3284671532846715\n",
      "Epoch:213, Accuracy:0.3248175182481752\n",
      "Epoch:214, Accuracy:0.29927007299270075\n",
      "Epoch:215, Accuracy:0.2956204379562044\n",
      "Epoch:216, Accuracy:0.2956204379562044\n",
      "Epoch:217, Accuracy:0.30656934306569344\n",
      "Epoch:218, Accuracy:0.3175182481751825\n",
      "Epoch:219, Accuracy:0.3284671532846715\n",
      "Epoch:220, Accuracy:0.31386861313868614\n",
      "Epoch:221, Accuracy:0.31386861313868614\n",
      "Epoch:222, Accuracy:0.3722627737226277\n",
      "Epoch:223, Accuracy:0.3722627737226277\n",
      "Epoch:224, Accuracy:0.35766423357664234\n",
      "Epoch:225, Accuracy:0.3795620437956204\n",
      "Epoch:226, Accuracy:0.31386861313868614\n",
      "Epoch:227, Accuracy:0.33211678832116787\n",
      "Epoch:228, Accuracy:0.3248175182481752\n",
      "Epoch:229, Accuracy:0.3248175182481752\n",
      "Epoch:230, Accuracy:0.33211678832116787\n",
      "Epoch:231, Accuracy:0.33211678832116787\n",
      "Epoch:232, Accuracy:0.3357664233576642\n",
      "Epoch:233, Accuracy:0.35036496350364965\n",
      "Epoch:234, Accuracy:0.34306569343065696\n",
      "Epoch:235, Accuracy:0.35036496350364965\n",
      "Epoch:236, Accuracy:0.3467153284671533\n",
      "Epoch:237, Accuracy:0.2846715328467153\n",
      "Epoch:238, Accuracy:0.2773722627737226\n",
      "Epoch:239, Accuracy:0.29927007299270075\n",
      "Epoch:240, Accuracy:0.2956204379562044\n",
      "Epoch:241, Accuracy:0.3102189781021898\n",
      "Epoch:242, Accuracy:0.33211678832116787\n",
      "Epoch:243, Accuracy:0.33211678832116787\n",
      "Epoch:244, Accuracy:0.33211678832116787\n",
      "Epoch:245, Accuracy:0.3467153284671533\n",
      "Epoch:246, Accuracy:0.3467153284671533\n",
      "Epoch:247, Accuracy:0.3613138686131387\n",
      "Epoch:248, Accuracy:0.36496350364963503\n",
      "Epoch:249, Accuracy:0.35036496350364965\n",
      "Epoch:250, Accuracy:0.3795620437956204\n",
      "Epoch:251, Accuracy:0.3795620437956204\n",
      "Epoch:252, Accuracy:0.3686131386861314\n",
      "Epoch:253, Accuracy:0.38686131386861317\n",
      "Epoch:254, Accuracy:0.3722627737226277\n",
      "Epoch:255, Accuracy:0.3467153284671533\n",
      "Epoch:256, Accuracy:0.34306569343065696\n",
      "Epoch:257, Accuracy:0.36496350364963503\n",
      "Epoch:258, Accuracy:0.3795620437956204\n",
      "Epoch:259, Accuracy:0.3905109489051095\n",
      "Epoch:260, Accuracy:0.3686131386861314\n",
      "Epoch:261, Accuracy:0.40145985401459855\n",
      "Epoch:262, Accuracy:0.35766423357664234\n",
      "Epoch:263, Accuracy:0.3978102189781022\n",
      "Epoch:264, Accuracy:0.3613138686131387\n",
      "Epoch:265, Accuracy:0.3686131386861314\n",
      "Epoch:266, Accuracy:0.3795620437956204\n",
      "Epoch:267, Accuracy:0.38686131386861317\n",
      "Epoch:268, Accuracy:0.3722627737226277\n",
      "Epoch:269, Accuracy:0.3686131386861314\n",
      "Epoch:270, Accuracy:0.3978102189781022\n",
      "Epoch:271, Accuracy:0.3978102189781022\n",
      "Epoch:272, Accuracy:0.4051094890510949\n",
      "Epoch:273, Accuracy:0.3795620437956204\n",
      "Epoch:274, Accuracy:0.3795620437956204\n",
      "Epoch:275, Accuracy:0.38686131386861317\n",
      "Epoch:276, Accuracy:0.38686131386861317\n",
      "Epoch:277, Accuracy:0.3795620437956204\n",
      "Epoch:278, Accuracy:0.3795620437956204\n",
      "Epoch:279, Accuracy:0.3905109489051095\n",
      "Epoch:280, Accuracy:0.3722627737226277\n",
      "Epoch:281, Accuracy:0.38321167883211676\n",
      "Epoch:282, Accuracy:0.38321167883211676\n",
      "Epoch:283, Accuracy:0.38686131386861317\n",
      "Epoch:284, Accuracy:0.3759124087591241\n",
      "Epoch:285, Accuracy:0.3722627737226277\n",
      "Epoch:286, Accuracy:0.3978102189781022\n",
      "Epoch:287, Accuracy:0.38321167883211676\n",
      "Epoch:288, Accuracy:0.3284671532846715\n",
      "Epoch:289, Accuracy:0.34306569343065696\n",
      "Epoch:290, Accuracy:0.3357664233576642\n",
      "Epoch:291, Accuracy:0.34306569343065696\n",
      "Epoch:292, Accuracy:0.35766423357664234\n",
      "Epoch:293, Accuracy:0.3613138686131387\n",
      "Epoch:294, Accuracy:0.3175182481751825\n",
      "Epoch:295, Accuracy:0.3284671532846715\n",
      "Epoch:296, Accuracy:0.2846715328467153\n",
      "Epoch:297, Accuracy:0.2846715328467153\n",
      "Epoch:298, Accuracy:0.32116788321167883\n",
      "Epoch:299, Accuracy:0.291970802919708\n",
      "Epoch:300, Accuracy:0.30656934306569344\n",
      "Epoch:301, Accuracy:0.29927007299270075\n",
      "Epoch:302, Accuracy:0.31386861313868614\n",
      "Epoch:303, Accuracy:0.3175182481751825\n",
      "Epoch:304, Accuracy:0.3175182481751825\n",
      "Epoch:305, Accuracy:0.34306569343065696\n",
      "Epoch:306, Accuracy:0.3357664233576642\n",
      "Epoch:307, Accuracy:0.17518248175182483\n",
      "Epoch:308, Accuracy:0.3175182481751825\n",
      "Epoch:309, Accuracy:0.31386861313868614\n",
      "Epoch:310, Accuracy:0.3248175182481752\n",
      "Epoch:311, Accuracy:0.29927007299270075\n",
      "Epoch:312, Accuracy:0.3102189781021898\n",
      "Epoch:313, Accuracy:0.31386861313868614\n",
      "Epoch:314, Accuracy:0.33941605839416056\n",
      "Epoch:315, Accuracy:0.33211678832116787\n",
      "Epoch:316, Accuracy:0.354014598540146\n",
      "Epoch:317, Accuracy:0.33211678832116787\n",
      "Epoch:318, Accuracy:0.3357664233576642\n",
      "Epoch:319, Accuracy:0.34306569343065696\n",
      "Epoch:320, Accuracy:0.3284671532846715\n",
      "Epoch:321, Accuracy:0.3284671532846715\n",
      "Epoch:322, Accuracy:0.33941605839416056\n",
      "Epoch:323, Accuracy:0.33211678832116787\n",
      "Epoch:324, Accuracy:0.3759124087591241\n",
      "Epoch:325, Accuracy:0.3795620437956204\n",
      "Epoch:326, Accuracy:0.38686131386861317\n",
      "Epoch:327, Accuracy:0.3905109489051095\n",
      "Epoch:328, Accuracy:0.35036496350364965\n",
      "Epoch:329, Accuracy:0.36496350364963503\n",
      "Epoch:330, Accuracy:0.354014598540146\n",
      "Epoch:331, Accuracy:0.3613138686131387\n",
      "Epoch:332, Accuracy:0.3613138686131387\n",
      "Epoch:333, Accuracy:0.38321167883211676\n",
      "Epoch:334, Accuracy:0.35036496350364965\n",
      "Epoch:335, Accuracy:0.3795620437956204\n",
      "Epoch:336, Accuracy:0.38321167883211676\n",
      "Epoch:337, Accuracy:0.39416058394160586\n",
      "Epoch:338, Accuracy:0.3795620437956204\n",
      "Epoch:339, Accuracy:0.4124087591240876\n",
      "Epoch:340, Accuracy:0.40145985401459855\n",
      "Epoch:341, Accuracy:0.40145985401459855\n",
      "Epoch:342, Accuracy:0.3795620437956204\n",
      "Epoch:343, Accuracy:0.3978102189781022\n",
      "Epoch:344, Accuracy:0.39416058394160586\n",
      "Epoch:345, Accuracy:0.35766423357664234\n",
      "Epoch:346, Accuracy:0.33941605839416056\n",
      "Epoch:347, Accuracy:0.3759124087591241\n",
      "Epoch:348, Accuracy:0.38321167883211676\n",
      "Epoch:349, Accuracy:0.38321167883211676\n",
      "Epoch:350, Accuracy:0.38686131386861317\n",
      "Epoch:351, Accuracy:0.40875912408759124\n",
      "Epoch:352, Accuracy:0.3978102189781022\n",
      "Epoch:353, Accuracy:0.3978102189781022\n",
      "Epoch:354, Accuracy:0.3905109489051095\n",
      "Epoch:355, Accuracy:0.39416058394160586\n",
      "Epoch:356, Accuracy:0.39416058394160586\n",
      "Epoch:357, Accuracy:0.3795620437956204\n",
      "Epoch:358, Accuracy:0.4051094890510949\n",
      "Epoch:359, Accuracy:0.41605839416058393\n",
      "Epoch:360, Accuracy:0.40875912408759124\n",
      "Epoch:361, Accuracy:0.41605839416058393\n",
      "Epoch:362, Accuracy:0.4124087591240876\n",
      "Epoch:363, Accuracy:0.40875912408759124\n",
      "Epoch:364, Accuracy:0.41605839416058393\n",
      "Epoch:365, Accuracy:0.4197080291970803\n",
      "Epoch:366, Accuracy:0.40875912408759124\n",
      "Epoch:367, Accuracy:0.41605839416058393\n",
      "Epoch:368, Accuracy:0.4306569343065693\n",
      "Epoch:369, Accuracy:0.4343065693430657\n",
      "Epoch:370, Accuracy:0.41605839416058393\n",
      "Epoch:371, Accuracy:0.43795620437956206\n",
      "Epoch:372, Accuracy:0.41605839416058393\n",
      "Epoch:373, Accuracy:0.4306569343065693\n",
      "Epoch:374, Accuracy:0.43795620437956206\n",
      "Epoch:375, Accuracy:0.4306569343065693\n",
      "Epoch:376, Accuracy:0.40145985401459855\n",
      "Epoch:377, Accuracy:0.41605839416058393\n",
      "Epoch:378, Accuracy:0.43795620437956206\n",
      "Epoch:379, Accuracy:0.4343065693430657\n",
      "Epoch:380, Accuracy:0.4233576642335766\n",
      "Epoch:381, Accuracy:0.4306569343065693\n",
      "Epoch:382, Accuracy:0.42700729927007297\n",
      "Epoch:383, Accuracy:0.42700729927007297\n",
      "Epoch:384, Accuracy:0.3795620437956204\n",
      "Epoch:385, Accuracy:0.4343065693430657\n",
      "Epoch:386, Accuracy:0.4124087591240876\n",
      "Epoch:387, Accuracy:0.4233576642335766\n",
      "Epoch:388, Accuracy:0.4343065693430657\n",
      "Epoch:389, Accuracy:0.4051094890510949\n",
      "Epoch:390, Accuracy:0.3759124087591241\n",
      "Epoch:391, Accuracy:0.3759124087591241\n",
      "Epoch:392, Accuracy:0.3759124087591241\n",
      "Epoch:393, Accuracy:0.38321167883211676\n",
      "Epoch:394, Accuracy:0.40875912408759124\n",
      "Epoch:395, Accuracy:0.3613138686131387\n",
      "Epoch:396, Accuracy:0.38686131386861317\n",
      "Epoch:397, Accuracy:0.3978102189781022\n",
      "Epoch:398, Accuracy:0.3759124087591241\n",
      "Epoch:399, Accuracy:0.39416058394160586\n",
      "Epoch:400, Accuracy:0.38686131386861317\n",
      "Epoch:401, Accuracy:0.3905109489051095\n",
      "Epoch:402, Accuracy:0.40145985401459855\n",
      "Epoch:403, Accuracy:0.41605839416058393\n",
      "Epoch:404, Accuracy:0.4197080291970803\n",
      "Epoch:405, Accuracy:0.4233576642335766\n",
      "Epoch:406, Accuracy:0.42700729927007297\n",
      "Epoch:407, Accuracy:0.42700729927007297\n",
      "Epoch:408, Accuracy:0.4197080291970803\n",
      "Epoch:409, Accuracy:0.4233576642335766\n",
      "Epoch:410, Accuracy:0.4051094890510949\n",
      "Epoch:411, Accuracy:0.4343065693430657\n",
      "Epoch:412, Accuracy:0.4489051094890511\n",
      "Epoch:413, Accuracy:0.42700729927007297\n",
      "Epoch:414, Accuracy:0.44525547445255476\n",
      "Epoch:415, Accuracy:0.4562043795620438\n",
      "Epoch:416, Accuracy:0.46715328467153283\n",
      "Epoch:417, Accuracy:0.4744525547445255\n",
      "Epoch:418, Accuracy:0.46715328467153283\n",
      "Epoch:419, Accuracy:0.4854014598540146\n",
      "Epoch:420, Accuracy:0.48175182481751827\n",
      "Epoch:421, Accuracy:0.4927007299270073\n",
      "Epoch:422, Accuracy:0.48175182481751827\n",
      "Epoch:423, Accuracy:0.4927007299270073\n",
      "Epoch:424, Accuracy:0.4744525547445255\n",
      "Epoch:425, Accuracy:0.4708029197080292\n",
      "Epoch:426, Accuracy:0.4708029197080292\n",
      "Epoch:427, Accuracy:0.4562043795620438\n",
      "Epoch:428, Accuracy:0.48905109489051096\n",
      "Epoch:429, Accuracy:0.4708029197080292\n",
      "Epoch:430, Accuracy:0.48175182481751827\n",
      "Epoch:431, Accuracy:0.43795620437956206\n",
      "Epoch:432, Accuracy:0.4489051094890511\n",
      "Epoch:433, Accuracy:0.42700729927007297\n",
      "Epoch:434, Accuracy:0.38321167883211676\n",
      "Epoch:435, Accuracy:0.3905109489051095\n",
      "Epoch:436, Accuracy:0.4306569343065693\n",
      "Epoch:437, Accuracy:0.4416058394160584\n",
      "Epoch:438, Accuracy:0.4306569343065693\n",
      "Epoch:439, Accuracy:0.41605839416058393\n",
      "Epoch:440, Accuracy:0.3978102189781022\n",
      "Epoch:441, Accuracy:0.40875912408759124\n",
      "Epoch:442, Accuracy:0.43795620437956206\n",
      "Epoch:443, Accuracy:0.43795620437956206\n",
      "Epoch:444, Accuracy:0.44525547445255476\n",
      "Epoch:445, Accuracy:0.45255474452554745\n",
      "Epoch:446, Accuracy:0.45255474452554745\n",
      "Epoch:447, Accuracy:0.4489051094890511\n",
      "Epoch:448, Accuracy:0.46715328467153283\n",
      "Epoch:449, Accuracy:0.4744525547445255\n",
      "Epoch:450, Accuracy:0.4781021897810219\n",
      "Epoch:451, Accuracy:0.41605839416058393\n",
      "Epoch:452, Accuracy:0.41605839416058393\n",
      "Epoch:453, Accuracy:0.4197080291970803\n",
      "Epoch:454, Accuracy:0.4197080291970803\n",
      "Epoch:455, Accuracy:0.44525547445255476\n",
      "Epoch:456, Accuracy:0.4343065693430657\n",
      "Epoch:457, Accuracy:0.4306569343065693\n",
      "Epoch:458, Accuracy:0.43795620437956206\n",
      "Epoch:459, Accuracy:0.44525547445255476\n",
      "Epoch:460, Accuracy:0.44525547445255476\n",
      "Epoch:461, Accuracy:0.45985401459854014\n",
      "Epoch:462, Accuracy:0.46715328467153283\n",
      "Epoch:463, Accuracy:0.4854014598540146\n",
      "Epoch:464, Accuracy:0.48905109489051096\n",
      "Epoch:465, Accuracy:0.4744525547445255\n",
      "Epoch:466, Accuracy:0.45255474452554745\n",
      "Epoch:467, Accuracy:0.4562043795620438\n",
      "Epoch:468, Accuracy:0.44525547445255476\n",
      "Epoch:469, Accuracy:0.45255474452554745\n",
      "Epoch:470, Accuracy:0.44525547445255476\n",
      "Epoch:471, Accuracy:0.4635036496350365\n",
      "Epoch:472, Accuracy:0.4489051094890511\n",
      "Epoch:473, Accuracy:0.4306569343065693\n",
      "Epoch:474, Accuracy:0.4233576642335766\n",
      "Epoch:475, Accuracy:0.4306569343065693\n",
      "Epoch:476, Accuracy:0.4416058394160584\n",
      "Epoch:477, Accuracy:0.43795620437956206\n",
      "Epoch:478, Accuracy:0.45985401459854014\n",
      "Epoch:479, Accuracy:0.4708029197080292\n",
      "Epoch:480, Accuracy:0.49635036496350365\n",
      "Epoch:481, Accuracy:0.46715328467153283\n",
      "Epoch:482, Accuracy:0.5\n",
      "Epoch:483, Accuracy:0.5145985401459854\n",
      "Epoch:484, Accuracy:0.5109489051094891\n",
      "Epoch:485, Accuracy:0.5036496350364964\n",
      "Epoch:486, Accuracy:0.5\n",
      "Epoch:487, Accuracy:0.49635036496350365\n",
      "Epoch:488, Accuracy:0.4927007299270073\n",
      "Epoch:489, Accuracy:0.5\n",
      "Epoch:490, Accuracy:0.5036496350364964\n",
      "Epoch:491, Accuracy:0.5\n",
      "Epoch:492, Accuracy:0.5255474452554745\n",
      "Epoch:493, Accuracy:0.5218978102189781\n",
      "Epoch:494, Accuracy:0.5109489051094891\n",
      "Epoch:495, Accuracy:0.5109489051094891\n",
      "Epoch:496, Accuracy:0.5\n",
      "Epoch:497, Accuracy:0.5\n",
      "Epoch:498, Accuracy:0.49635036496350365\n",
      "Epoch:499, Accuracy:0.5\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "if torch.cuda.is_available():\n",
    "    model = CNNLargeNet2().cuda()\n",
    "Y = trainSmall(model, train_dataset, batch_size=64, learning_rate=0.01, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:49:06.164585684Z",
     "start_time": "2023-11-12T19:46:09.045451655Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUfbTExPwd0U",
    "outputId": "162126e6-ea50-459e-c867-f6dfa2169b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Start\n",
      "Epoch:0, Accuracy:0.16058394160583941\n",
      "Epoch:1, Accuracy:0.0948905109489051\n",
      "Epoch:2, Accuracy:0.16058394160583941\n",
      "Epoch:3, Accuracy:0.17518248175182483\n",
      "Epoch:4, Accuracy:0.18613138686131386\n",
      "Epoch:5, Accuracy:0.17518248175182483\n",
      "Epoch:6, Accuracy:0.18613138686131386\n",
      "Epoch:7, Accuracy:0.18613138686131386\n",
      "Epoch:8, Accuracy:0.1897810218978102\n",
      "Epoch:9, Accuracy:0.20437956204379562\n",
      "Epoch:10, Accuracy:0.20437956204379562\n",
      "Epoch:11, Accuracy:0.20802919708029197\n",
      "Epoch:12, Accuracy:0.20802919708029197\n",
      "Epoch:13, Accuracy:0.20437956204379562\n",
      "Epoch:14, Accuracy:0.21897810218978103\n",
      "Epoch:15, Accuracy:0.22992700729927007\n",
      "Epoch:16, Accuracy:0.23722627737226276\n",
      "Epoch:17, Accuracy:0.2518248175182482\n",
      "Epoch:18, Accuracy:0.24817518248175183\n",
      "Epoch:19, Accuracy:0.27007299270072993\n",
      "Epoch:20, Accuracy:0.2664233576642336\n",
      "Epoch:21, Accuracy:0.2591240875912409\n",
      "Epoch:22, Accuracy:0.24817518248175183\n",
      "Epoch:23, Accuracy:0.24452554744525548\n",
      "Epoch:24, Accuracy:0.2591240875912409\n",
      "Epoch:25, Accuracy:0.20437956204379562\n",
      "Epoch:26, Accuracy:0.33211678832116787\n",
      "Epoch:27, Accuracy:0.23722627737226276\n",
      "Epoch:28, Accuracy:0.2591240875912409\n",
      "Epoch:29, Accuracy:0.27007299270072993\n",
      "Epoch:30, Accuracy:0.27007299270072993\n",
      "Epoch:31, Accuracy:0.2846715328467153\n",
      "Epoch:32, Accuracy:0.2956204379562044\n",
      "Epoch:33, Accuracy:0.2956204379562044\n",
      "Epoch:34, Accuracy:0.3722627737226277\n",
      "Epoch:35, Accuracy:0.32116788321167883\n",
      "Epoch:36, Accuracy:0.3978102189781022\n",
      "Epoch:37, Accuracy:0.3686131386861314\n",
      "Epoch:38, Accuracy:0.3795620437956204\n",
      "Epoch:39, Accuracy:0.4124087591240876\n",
      "Epoch:40, Accuracy:0.42700729927007297\n",
      "Epoch:41, Accuracy:0.45255474452554745\n",
      "Epoch:42, Accuracy:0.5036496350364964\n",
      "Epoch:43, Accuracy:0.43795620437956206\n",
      "Epoch:44, Accuracy:0.5036496350364964\n",
      "Epoch:45, Accuracy:0.5291970802919708\n",
      "Epoch:46, Accuracy:0.41605839416058393\n",
      "Epoch:47, Accuracy:0.43795620437956206\n",
      "Epoch:48, Accuracy:0.4416058394160584\n",
      "Epoch:49, Accuracy:0.4635036496350365\n",
      "Epoch:50, Accuracy:0.4562043795620438\n",
      "Epoch:51, Accuracy:0.5182481751824818\n",
      "Epoch:52, Accuracy:0.4562043795620438\n",
      "Epoch:53, Accuracy:0.572992700729927\n",
      "Epoch:54, Accuracy:0.5985401459854015\n",
      "Epoch:55, Accuracy:0.5291970802919708\n",
      "Epoch:56, Accuracy:0.6277372262773723\n",
      "Epoch:57, Accuracy:0.6386861313868614\n",
      "Epoch:58, Accuracy:0.6605839416058394\n",
      "Epoch:59, Accuracy:0.6715328467153284\n",
      "Epoch:60, Accuracy:0.5766423357664233\n",
      "Epoch:61, Accuracy:0.6678832116788321\n",
      "Epoch:62, Accuracy:0.6167883211678832\n",
      "Epoch:63, Accuracy:0.45985401459854014\n",
      "Epoch:64, Accuracy:0.5\n",
      "Epoch:65, Accuracy:0.3686131386861314\n",
      "Epoch:66, Accuracy:0.30656934306569344\n",
      "Epoch:67, Accuracy:0.3905109489051095\n",
      "Epoch:68, Accuracy:0.39416058394160586\n",
      "Epoch:69, Accuracy:0.38321167883211676\n",
      "Epoch:70, Accuracy:0.4306569343065693\n",
      "Epoch:71, Accuracy:0.4781021897810219\n",
      "Epoch:72, Accuracy:0.49635036496350365\n",
      "Epoch:73, Accuracy:0.5036496350364964\n",
      "Epoch:74, Accuracy:0.551094890510949\n",
      "Epoch:75, Accuracy:0.5437956204379562\n",
      "Epoch:76, Accuracy:0.5656934306569343\n",
      "Epoch:77, Accuracy:0.5948905109489051\n",
      "Epoch:78, Accuracy:0.6532846715328468\n",
      "Epoch:79, Accuracy:0.6496350364963503\n",
      "Epoch:80, Accuracy:0.6788321167883211\n",
      "Epoch:81, Accuracy:0.7043795620437956\n",
      "Epoch:82, Accuracy:0.6605839416058394\n",
      "Epoch:83, Accuracy:0.48175182481751827\n",
      "Epoch:84, Accuracy:0.708029197080292\n",
      "Epoch:85, Accuracy:0.6167883211678832\n",
      "Epoch:86, Accuracy:0.6094890510948905\n",
      "Epoch:87, Accuracy:0.6131386861313869\n",
      "Epoch:88, Accuracy:0.4708029197080292\n",
      "Epoch:89, Accuracy:0.5401459854014599\n",
      "Epoch:90, Accuracy:0.6131386861313869\n",
      "Epoch:91, Accuracy:0.5255474452554745\n",
      "Epoch:92, Accuracy:0.6167883211678832\n",
      "Epoch:93, Accuracy:0.6423357664233577\n",
      "Epoch:94, Accuracy:0.6970802919708029\n",
      "Epoch:95, Accuracy:0.6058394160583942\n",
      "Epoch:96, Accuracy:0.7408759124087592\n",
      "Epoch:97, Accuracy:0.6094890510948905\n",
      "Epoch:98, Accuracy:0.5875912408759124\n",
      "Epoch:99, Accuracy:0.6423357664233577\n",
      "Epoch:100, Accuracy:0.6094890510948905\n",
      "Epoch:101, Accuracy:0.635036496350365\n",
      "Epoch:102, Accuracy:0.7627737226277372\n",
      "Epoch:103, Accuracy:0.7627737226277372\n",
      "Epoch:104, Accuracy:0.8065693430656934\n",
      "Epoch:105, Accuracy:0.8102189781021898\n",
      "Epoch:106, Accuracy:0.8357664233576643\n",
      "Epoch:107, Accuracy:0.8211678832116789\n",
      "Epoch:108, Accuracy:0.8686131386861314\n",
      "Epoch:109, Accuracy:0.8321167883211679\n",
      "Epoch:110, Accuracy:0.864963503649635\n",
      "Epoch:111, Accuracy:0.8613138686131386\n",
      "Epoch:112, Accuracy:0.8576642335766423\n",
      "Epoch:113, Accuracy:0.7299270072992701\n",
      "Epoch:114, Accuracy:0.7591240875912408\n",
      "Epoch:115, Accuracy:0.5291970802919708\n",
      "Epoch:116, Accuracy:0.8175182481751825\n",
      "Epoch:117, Accuracy:0.7481751824817519\n",
      "Epoch:118, Accuracy:0.8540145985401459\n",
      "Epoch:119, Accuracy:0.8065693430656934\n",
      "Epoch:120, Accuracy:0.8321167883211679\n",
      "Epoch:121, Accuracy:0.7737226277372263\n",
      "Epoch:122, Accuracy:0.781021897810219\n",
      "Epoch:123, Accuracy:0.708029197080292\n",
      "Epoch:124, Accuracy:0.6934306569343066\n",
      "Epoch:125, Accuracy:0.7700729927007299\n",
      "Epoch:126, Accuracy:0.8029197080291971\n",
      "Epoch:127, Accuracy:0.8795620437956204\n",
      "Epoch:128, Accuracy:0.8832116788321168\n",
      "Epoch:129, Accuracy:0.9197080291970803\n",
      "Epoch:130, Accuracy:0.948905109489051\n",
      "Epoch:131, Accuracy:0.9525547445255474\n",
      "Epoch:132, Accuracy:0.9744525547445255\n",
      "Epoch:133, Accuracy:0.9817518248175182\n",
      "Epoch:134, Accuracy:0.9817518248175182\n",
      "Epoch:135, Accuracy:0.9890510948905109\n",
      "Epoch:136, Accuracy:0.9890510948905109\n",
      "Epoch:137, Accuracy:0.9890510948905109\n",
      "Epoch:138, Accuracy:0.9890510948905109\n",
      "Epoch:139, Accuracy:0.9927007299270073\n",
      "Epoch:140, Accuracy:0.9927007299270073\n",
      "Epoch:141, Accuracy:0.9927007299270073\n",
      "Epoch:142, Accuracy:0.9963503649635036\n",
      "Epoch:143, Accuracy:1.0\n",
      "Epoch:144, Accuracy:1.0\n",
      "Epoch:145, Accuracy:1.0\n",
      "Epoch:146, Accuracy:1.0\n",
      "Epoch:147, Accuracy:1.0\n",
      "Epoch:148, Accuracy:1.0\n",
      "Epoch:149, Accuracy:1.0\n",
      "Epoch:150, Accuracy:1.0\n",
      "Epoch:151, Accuracy:1.0\n",
      "Epoch:152, Accuracy:1.0\n",
      "Epoch:153, Accuracy:1.0\n",
      "Epoch:154, Accuracy:1.0\n",
      "Epoch:155, Accuracy:1.0\n",
      "Epoch:156, Accuracy:1.0\n",
      "Epoch:157, Accuracy:1.0\n",
      "Epoch:158, Accuracy:1.0\n",
      "Epoch:159, Accuracy:1.0\n",
      "Epoch:160, Accuracy:1.0\n",
      "Epoch:161, Accuracy:1.0\n",
      "Epoch:162, Accuracy:1.0\n",
      "Epoch:163, Accuracy:1.0\n",
      "Epoch:164, Accuracy:1.0\n",
      "Epoch:165, Accuracy:1.0\n",
      "Epoch:166, Accuracy:1.0\n",
      "Epoch:167, Accuracy:1.0\n",
      "Epoch:168, Accuracy:1.0\n",
      "Epoch:169, Accuracy:1.0\n",
      "Epoch:170, Accuracy:1.0\n",
      "Epoch:171, Accuracy:1.0\n",
      "Epoch:172, Accuracy:1.0\n",
      "Epoch:173, Accuracy:1.0\n",
      "Epoch:174, Accuracy:1.0\n",
      "Epoch:175, Accuracy:1.0\n",
      "Epoch:176, Accuracy:1.0\n",
      "Epoch:177, Accuracy:1.0\n",
      "Epoch:178, Accuracy:1.0\n",
      "Epoch:179, Accuracy:1.0\n",
      "Epoch:180, Accuracy:1.0\n",
      "Epoch:181, Accuracy:1.0\n",
      "Epoch:182, Accuracy:1.0\n",
      "Epoch:183, Accuracy:1.0\n",
      "Epoch:184, Accuracy:1.0\n",
      "Epoch:185, Accuracy:1.0\n",
      "Epoch:186, Accuracy:1.0\n",
      "Epoch:187, Accuracy:1.0\n",
      "Epoch:188, Accuracy:1.0\n",
      "Epoch:189, Accuracy:1.0\n",
      "Epoch:190, Accuracy:1.0\n",
      "Epoch:191, Accuracy:1.0\n",
      "Epoch:192, Accuracy:1.0\n",
      "Epoch:193, Accuracy:1.0\n",
      "Epoch:194, Accuracy:1.0\n",
      "Epoch:195, Accuracy:1.0\n",
      "Epoch:196, Accuracy:1.0\n",
      "Epoch:197, Accuracy:1.0\n",
      "Epoch:198, Accuracy:1.0\n",
      "Epoch:199, Accuracy:1.0\n",
      "Epoch:200, Accuracy:1.0\n",
      "Epoch:201, Accuracy:1.0\n",
      "Epoch:202, Accuracy:1.0\n",
      "Epoch:203, Accuracy:1.0\n",
      "Epoch:204, Accuracy:1.0\n",
      "Epoch:205, Accuracy:1.0\n",
      "Epoch:206, Accuracy:1.0\n",
      "Epoch:207, Accuracy:1.0\n",
      "Epoch:208, Accuracy:1.0\n",
      "Epoch:209, Accuracy:1.0\n",
      "Epoch:210, Accuracy:1.0\n",
      "Epoch:211, Accuracy:1.0\n",
      "Epoch:212, Accuracy:1.0\n",
      "Epoch:213, Accuracy:1.0\n",
      "Epoch:214, Accuracy:1.0\n",
      "Epoch:215, Accuracy:1.0\n",
      "Epoch:216, Accuracy:1.0\n",
      "Epoch:217, Accuracy:1.0\n",
      "Epoch:218, Accuracy:1.0\n",
      "Epoch:219, Accuracy:1.0\n",
      "Epoch:220, Accuracy:1.0\n",
      "Epoch:221, Accuracy:1.0\n",
      "Epoch:222, Accuracy:1.0\n",
      "Epoch:223, Accuracy:1.0\n",
      "Epoch:224, Accuracy:1.0\n",
      "Epoch:225, Accuracy:1.0\n",
      "Epoch:226, Accuracy:1.0\n",
      "Epoch:227, Accuracy:1.0\n",
      "Epoch:228, Accuracy:1.0\n",
      "Epoch:229, Accuracy:1.0\n",
      "Epoch:230, Accuracy:1.0\n",
      "Epoch:231, Accuracy:1.0\n",
      "Epoch:232, Accuracy:1.0\n",
      "Epoch:233, Accuracy:1.0\n",
      "Epoch:234, Accuracy:1.0\n",
      "Epoch:235, Accuracy:1.0\n",
      "Epoch:236, Accuracy:1.0\n",
      "Epoch:237, Accuracy:1.0\n",
      "Epoch:238, Accuracy:1.0\n",
      "Epoch:239, Accuracy:1.0\n",
      "Epoch:240, Accuracy:1.0\n",
      "Epoch:241, Accuracy:1.0\n",
      "Epoch:242, Accuracy:1.0\n",
      "Epoch:243, Accuracy:1.0\n",
      "Epoch:244, Accuracy:1.0\n",
      "Epoch:245, Accuracy:1.0\n",
      "Epoch:246, Accuracy:1.0\n",
      "Epoch:247, Accuracy:1.0\n",
      "Epoch:248, Accuracy:1.0\n",
      "Epoch:249, Accuracy:1.0\n",
      "Epoch:250, Accuracy:1.0\n",
      "Epoch:251, Accuracy:1.0\n",
      "Epoch:252, Accuracy:1.0\n",
      "Epoch:253, Accuracy:1.0\n",
      "Epoch:254, Accuracy:1.0\n",
      "Epoch:255, Accuracy:1.0\n",
      "Epoch:256, Accuracy:1.0\n",
      "Epoch:257, Accuracy:1.0\n",
      "Epoch:258, Accuracy:1.0\n",
      "Epoch:259, Accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "if torch.cuda.is_available():\n",
    "    model = CNNLargeNet2().cuda()\n",
    "Y = trainSmall(model, train_dataset, batch_size=64, learning_rate=0.005, num_epochs=260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "In the last adjustment of hyperparameters, our training results showed overfitting. This indicates that our model is ready for the next step of training. In the following code, we rewrote the data loader and training function, and added checkpoints for each epoch. We attempted to use 60% of the entire dataset as training data, 20% as validation data, and the remaining 20% as test data.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T19:49:06.167753710Z",
     "start_time": "2023-11-12T19:49:06.167378697Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Label 0: 890 images\n",
      "Label 1: 208 images\n",
      "Label 2: 439 images\n",
      "Label 3: 570 images\n",
      "Label 4: 1406 images\n",
      "Label 5: 524 images\n",
      "Label 6: 746 images\n",
      "Label 7: 775 images\n"
     ]
    }
   ],
   "source": [
    "features_or = []\n",
    "labels_or = []\n",
    "train_data_or = []\n",
    "data_transform1 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "def create_train2():\n",
    "    for i, photo in enumerate(classeses):\n",
    "        path = os.path.join('C:/Users/Admin/Desktop/MIE1517_Project/data', photo)\n",
    "        label = i  # Use the index of the class as the label\n",
    "\n",
    "        # Loop over to get every image in the current class\n",
    "        for img_name in os.listdir(path):\n",
    "            img_path = os.path.join(path, img_name)\n",
    "            img_pil = cv.imread(img_path)\n",
    "            img_pil = Image.fromarray(cv.cvtColor(img_pil, cv.COLOR_BGR2RGB))\n",
    "            transformed_image1 = data_transform1(img_pil)\n",
    "\n",
    "            if img_pil is None:\n",
    "                print(\"Error loading image:\", img_pil)\n",
    "                continue\n",
    "            # Append the original image and its label to the list\n",
    "            features_or.append(transformed_image1)\n",
    "            labels_or.append(label)\n",
    "\n",
    "            train_data_or.append([img_pil, label])\n",
    "create_train2()\n",
    "print(type(features_or[0]))\n",
    "# Count the occurrences of each label\n",
    "label_counts_or = Counter(labels_or)\n",
    "\n",
    "# Print the label counts\n",
    "for label, count in label_counts_or.items():\n",
    "    print(f\"Label {label}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels_or\n",
    "features = features_or\n",
    "# Shuffle the labels and features in the same order\n",
    "combined = list(zip(labels, features))\n",
    "labels, features = zip(*combined)\n",
    "\n",
    "# Create a dictionary to store images by label\n",
    "label_to_images = defaultdict(list)\n",
    "\n",
    "# Group images by label\n",
    "for label, img in zip(labels, features):\n",
    "    label_to_images[label].append(img)\n",
    "\n",
    "# Set the ratios for train, validation, and test sets\n",
    "train_ratio = 0.6\n",
    "validation_ratio = 0.2\n",
    "test_ratio = 0.2\n",
    "\n",
    "train_data = []\n",
    "validation_data = []\n",
    "test_data = []\n",
    "\n",
    "# Split data for each label\n",
    "for label, images in label_to_images.items():\n",
    "    random.shuffle(images)  # Shuffle the images for each label\n",
    "\n",
    "    num_images = len(images)\n",
    "    num_train = int(train_ratio * num_images)\n",
    "    num_validation = int(validation_ratio * num_images)\n",
    "    num_test = num_images - num_train - num_validation\n",
    "\n",
    "    train_data.extend([(img, label) for img in images[:num_train]])\n",
    "    validation_data.extend([(img, label) for img in images[num_train:num_train + num_validation]])\n",
    "    test_data.extend([(img, label) for img in images[num_train + num_validation:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLargeNet(nn.Module):\n",
    "    def __init__( self, output1=32, output2=64, output3=128, output4=256):\n",
    "        super(CNNLargeNet, self).__init__()\n",
    "        self.name = \"CNN\"\n",
    "        \n",
    "        # Define the sequential layers for feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            # Convolutional Layer 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Convolutional Layer 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Convolutional Layer 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Convolutional Layer 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            # Convolutional Layer 5\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        # Flatten the tensor for the fully connected layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 8)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)  # Pass the input through the feature layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor for the classifier\n",
    "        x = self.classifier(x)  # Pass the flattened tensor through the classifier\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name, batch_size, learning_rate, epoch)\n",
    "    return path\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    \"\"\" Evaluate the network \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "      if use_cuda and torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "      outputs = model(inputs)\n",
    "      predicted = outputs.max(1, keepdim=True)[1]\n",
    "      total += inputs.shape[0]\n",
    "      correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainmodel(model, train_dataset, val_dataset, batch=64, learningRate=0.01, num_epochs=300):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
    "    \n",
    "    model_checkpoint_path = 'C:/Users/Admin/Desktop/MIE1517_Project/output'  # You can change the directory as needed\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch, shuffle=False) \n",
    "\n",
    "    train_accuracy = np.zeros(num_epochs)\n",
    "    train_losses = np.zeros(num_epochs)\n",
    "    validation_accuracy = np.zeros(num_epochs)\n",
    "    validation_losses = np.zeros(num_epochs)\n",
    "    iters = []\n",
    "\n",
    "    # Check CUDA availability\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    n = 0  # Initialize the iteration counter\n",
    "    # Training\n",
    "    print(\"Training Begin...\\n\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train mode\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # Get the inputs\n",
    "            inputs, labels = data\n",
    "            # Set up for gpu running\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass, backward pass, and optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            n += 1\n",
    "            # Calculate loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_losses[epoch] = running_loss\n",
    "        train_accuracy[epoch] = evaluate(model, train_loader)\n",
    "        \n",
    "        # Evaluation mode\n",
    "        model.eval()  # Set the model to evaluation mode for accuracy computation\n",
    "        valid_loss = 0.0\n",
    "        # Running without gradients are computed and model weights update\n",
    "        for inputs, labels in val_loader:\n",
    "            if use_cuda and torch.cuda.is_available():\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        validation_losses[epoch] = valid_loss\n",
    "        validation_accuracy[epoch] = evaluate(model, val_loader)\n",
    "        iters.append(n)\n",
    "        \n",
    "        # Print progress for the epoch\n",
    "        print((\"Epoch {}: Train Accuracy: {}, Train loss: {} |\"+\n",
    "               \"Validation Accuracy: {}, Validation loss: {}\").format(\n",
    "                   epoch + 1,\n",
    "                   train_accuracy[epoch],\n",
    "                   train_losses[epoch],\n",
    "                   validation_accuracy[epoch],\n",
    "                   validation_losses[epoch]))\n",
    "\n",
    "        # Save model checkpoint every 5 epochs\n",
    "        # if (epoch + 1) % 5 == 0:  # +1 because epochs start from 0\n",
    "            # Save the current model checkpoint to a file\n",
    "        model_path = get_model_name(model.name, batch, learningRate, epoch)\n",
    "        model_path = os.path.join(model_checkpoint_path, model_path)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    print(\"Finshied Training\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Total time elapsed: {:.2f} seconds\".format(elapsed_time))\n",
    "    # Write the train/test loss/err into CSV file for plotting later\n",
    "    # Save model checkpoint\n",
    "    epochs = np.arange(1, num_epochs + 1)\n",
    "    np.savetxt(\"{}_train_acc.csv\".format(model_path), train_accuracy)\n",
    "    np.savetxt(\"{}_train_loss.csv\".format(model_path), train_losses)\n",
    "    np.savetxt(\"{}_val_acc.csv\".format(model_path), validation_accuracy)\n",
    "    np.savetxt(\"{}_val_loss.csv\".format(model_path), validation_losses)\n",
    "\n",
    "    plt.title(\"Training Loss Curve\")\n",
    "    plt.plot(iters, train_losses, label=\"Train\")\n",
    "    plt.xlabel(\"iters\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Accuracy Curve\")\n",
    "    plt.plot(iters, train_accuracy, label=\"Train\")\n",
    "    plt.xlabel(\"iters\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.show()\n",
    "    print(\"Finished Training\")\n",
    "    return iters, train_losses, train_accuracy, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Begin...\n",
      "\n",
      "Epoch 1: Train Accuracy: 0.2530012004801921, Train loss: 448.71146273612976 |Validation Accuracy: 0.25067628494138866, Validation loss: 35.90981090068817\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\Project.ipynb Cell 24\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Project.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Project.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model \u001b[39m=\u001b[39m CNNLargeNet()\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Project.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m Y \u001b[39m=\u001b[39m trainmodel(model, train_data, validation_data, batch\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, learningRate\u001b[39m=\u001b[39m\u001b[39m0.005\u001b[39m, num_epochs\u001b[39m=\u001b[39m\u001b[39m520\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Admin\\Desktop\\Project.ipynb Cell 24\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Project.ipynb#X32sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     n \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Project.ipynb#X32sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# Calculate loss\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Project.ipynb#X32sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Project.ipynb#X32sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m train_losses[epoch] \u001b[39m=\u001b[39m running_loss\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Admin/Desktop/Project.ipynb#X32sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m train_accuracy[epoch] \u001b[39m=\u001b[39m evaluate(model, train_loader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "use_cuda = True\n",
    "if torch.cuda.is_available():\n",
    "    model = CNNLargeNet().cuda()\n",
    "Y = trainmodel(model, train_data, validation_data, batch=64, learningRate=0.005, num_epochs=520)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
